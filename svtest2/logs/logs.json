{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n    result = await delegate(prompt, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 80, in invoke\n    await self._handle_post_request_limiting(result)\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 57, in _handle_post_request_limiting\n    async with self._limiter.use(manifest):\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\limiting\\base.py\", line 32, in __aenter__\n    await self._limiter.acquire(self._manifest)\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\limiting\\composite.py\", line 33, in acquire\n    await limiter.acquire(manifest)\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\limiting\\tpm.py\", line 40, in acquire\n    await self._limiter.acquire(min(total_tokens, self._tokens_per_minute))\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\aiolimiter\\leakybucket.py\", line 151, in acquire\n    await fut\nasyncio.exceptions.CancelledError\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\graphrag\\index\\operations\\summarize_communities\\community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n               ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\graphrag\\callbacks\\workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\graphrag\\callbacks\\file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n          ^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 202, in encode\n    chunks = list(chunks)\n             ^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 432, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 439, in _iterencode\n    o = _default(o)\n        ^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 180, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n    result = await delegate(prompt, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 80, in invoke\n    await self._handle_post_request_limiting(result)\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 57, in _handle_post_request_limiting\n    async with self._limiter.use(manifest):\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\limiting\\base.py\", line 31, in __aenter__\n    async with LimitContext.acquire_semaphore:\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\asyncio\\locks.py\", line 15, in __aenter__\n    await self.acquire()\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\asyncio\\locks.py\", line 387, in acquire\n    await fut\nasyncio.exceptions.CancelledError\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\graphrag\\index\\operations\\summarize_communities\\community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n               ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\graphrag\\callbacks\\workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\graphrag\\callbacks\\file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n          ^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 202, in encode\n    chunks = list(chunks)\n             ^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 432, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 439, in _iterencode\n    o = _default(o)\n        ^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 180, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n    result = await delegate(prompt, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 80, in invoke\n    await self._handle_post_request_limiting(result)\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 57, in _handle_post_request_limiting\n    async with self._limiter.use(manifest):\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\limiting\\base.py\", line 31, in __aenter__\n    async with LimitContext.acquire_semaphore:\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\asyncio\\locks.py\", line 15, in __aenter__\n    await self.acquire()\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\asyncio\\locks.py\", line 387, in acquire\n    await fut\nasyncio.exceptions.CancelledError\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\graphrag\\index\\operations\\summarize_communities\\community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n               ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\graphrag\\callbacks\\workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\graphrag\\callbacks\\file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n          ^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 202, in encode\n    chunks = list(chunks)\n             ^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 432, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 439, in _iterencode\n    o = _default(o)\n        ^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 180, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n    result = await delegate(prompt, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 80, in invoke\n    await self._handle_post_request_limiting(result)\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 57, in _handle_post_request_limiting\n    async with self._limiter.use(manifest):\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\limiting\\base.py\", line 31, in __aenter__\n    async with LimitContext.acquire_semaphore:\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\asyncio\\locks.py\", line 15, in __aenter__\n    await self.acquire()\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\asyncio\\locks.py\", line 387, in acquire\n    await fut\nasyncio.exceptions.CancelledError\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\graphrag\\index\\operations\\summarize_communities\\community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n               ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\graphrag\\callbacks\\workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\graphrag\\callbacks\\file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n          ^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 202, in encode\n    chunks = list(chunks)\n             ^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 432, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 439, in _iterencode\n    o = _default(o)\n        ^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 180, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n    result = await delegate(prompt, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 80, in invoke\n    await self._handle_post_request_limiting(result)\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 57, in _handle_post_request_limiting\n    async with self._limiter.use(manifest):\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\limiting\\base.py\", line 31, in __aenter__\n    async with LimitContext.acquire_semaphore:\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\asyncio\\locks.py\", line 15, in __aenter__\n    await self.acquire()\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\asyncio\\locks.py\", line 387, in acquire\n    await fut\nasyncio.exceptions.CancelledError\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\graphrag\\index\\operations\\summarize_communities\\community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n               ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\graphrag\\callbacks\\workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\graphrag\\callbacks\\file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n          ^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 202, in encode\n    chunks = list(chunks)\n             ^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 432, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 439, in _iterencode\n    o = _default(o)\n        ^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 180, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n    result = await delegate(prompt, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 80, in invoke\n    await self._handle_post_request_limiting(result)\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 57, in _handle_post_request_limiting\n    async with self._limiter.use(manifest):\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\limiting\\base.py\", line 31, in __aenter__\n    async with LimitContext.acquire_semaphore:\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\asyncio\\locks.py\", line 15, in __aenter__\n    await self.acquire()\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\asyncio\\locks.py\", line 387, in acquire\n    await fut\nasyncio.exceptions.CancelledError\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\graphrag\\index\\operations\\summarize_communities\\community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n               ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\graphrag\\callbacks\\workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\graphrag\\callbacks\\file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n          ^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 202, in encode\n    chunks = list(chunks)\n             ^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 432, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 439, in _iterencode\n    o = _default(o)\n        ^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 180, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n    result = await delegate(prompt, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 80, in invoke\n    await self._handle_post_request_limiting(result)\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 57, in _handle_post_request_limiting\n    async with self._limiter.use(manifest):\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\limiting\\base.py\", line 31, in __aenter__\n    async with LimitContext.acquire_semaphore:\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\asyncio\\locks.py\", line 15, in __aenter__\n    await self.acquire()\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\asyncio\\locks.py\", line 387, in acquire\n    await fut\nasyncio.exceptions.CancelledError\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\graphrag\\index\\operations\\summarize_communities\\community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n               ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\graphrag\\callbacks\\workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\graphrag\\callbacks\\file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n          ^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 202, in encode\n    chunks = list(chunks)\n             ^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 432, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 439, in _iterencode\n    o = _default(o)\n        ^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 180, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n    result = await delegate(prompt, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 80, in invoke\n    await self._handle_post_request_limiting(result)\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 57, in _handle_post_request_limiting\n    async with self._limiter.use(manifest):\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\limiting\\base.py\", line 31, in __aenter__\n    async with LimitContext.acquire_semaphore:\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\asyncio\\locks.py\", line 15, in __aenter__\n    await self.acquire()\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\asyncio\\locks.py\", line 387, in acquire\n    await fut\nasyncio.exceptions.CancelledError\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\graphrag\\index\\operations\\summarize_communities\\community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n               ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\graphrag\\callbacks\\workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\graphrag\\callbacks\\file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n          ^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 202, in encode\n    chunks = list(chunks)\n             ^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 432, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 439, in _iterencode\n    o = _default(o)\n        ^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 180, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n    result = await delegate(prompt, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 73, in invoke\n    async with self._limiter.use(manifest):\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\limiting\\base.py\", line 32, in __aenter__\n    await self._limiter.acquire(self._manifest)\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\limiting\\composite.py\", line 33, in acquire\n    await limiter.acquire(manifest)\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\limiting\\tpm.py\", line 40, in acquire\n    await self._limiter.acquire(min(total_tokens, self._tokens_per_minute))\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\aiolimiter\\leakybucket.py\", line 151, in acquire\n    await fut\nasyncio.exceptions.CancelledError\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\graphrag\\index\\operations\\summarize_communities\\community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n               ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\graphrag\\callbacks\\workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\graphrag\\callbacks\\file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n          ^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 202, in encode\n    chunks = list(chunks)\n             ^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 432, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 439, in _iterencode\n    o = _default(o)\n        ^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 180, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n    result = await delegate(prompt, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 80, in invoke\n    await self._handle_post_request_limiting(result)\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 57, in _handle_post_request_limiting\n    async with self._limiter.use(manifest):\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\limiting\\base.py\", line 31, in __aenter__\n    async with LimitContext.acquire_semaphore:\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\asyncio\\locks.py\", line 15, in __aenter__\n    await self.acquire()\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\asyncio\\locks.py\", line 387, in acquire\n    await fut\nasyncio.exceptions.CancelledError\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\graphrag\\index\\operations\\summarize_communities\\community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n               ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\graphrag\\callbacks\\workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\graphrag\\callbacks\\file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n          ^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 202, in encode\n    chunks = list(chunks)\n             ^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 432, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 439, in _iterencode\n    o = _default(o)\n        ^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 180, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n    result = await delegate(prompt, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 80, in invoke\n    await self._handle_post_request_limiting(result)\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 57, in _handle_post_request_limiting\n    async with self._limiter.use(manifest):\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\limiting\\base.py\", line 31, in __aenter__\n    async with LimitContext.acquire_semaphore:\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\asyncio\\locks.py\", line 15, in __aenter__\n    await self.acquire()\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\asyncio\\locks.py\", line 387, in acquire\n    await fut\nasyncio.exceptions.CancelledError\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\graphrag\\index\\operations\\summarize_communities\\community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n               ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\graphrag\\callbacks\\workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\graphrag\\callbacks\\file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n          ^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 202, in encode\n    chunks = list(chunks)\n             ^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 432, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 439, in _iterencode\n    o = _default(o)\n        ^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 180, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 137, in invoke\n    result = await delegate(prompt, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 80, in invoke\n    await self._handle_post_request_limiting(result)\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 57, in _handle_post_request_limiting\n    async with self._limiter.use(manifest):\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\limiting\\base.py\", line 31, in __aenter__\n    async with LimitContext.acquire_semaphore:\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\asyncio\\locks.py\", line 15, in __aenter__\n    await self.acquire()\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\asyncio\\locks.py\", line 387, in acquire\n    await fut\nasyncio.exceptions.CancelledError\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\graphrag\\index\\operations\\summarize_communities\\community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n               ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\graphrag\\callbacks\\workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\site-packages\\graphrag\\callbacks\\file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n          ^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 202, in encode\n    chunks = list(chunks)\n             ^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 432, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 439, in _iterencode\n    o = _default(o)\n        ^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\GraphRAG2\\Lib\\json\\encoder.py\", line 180, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
